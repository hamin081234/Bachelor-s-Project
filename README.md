# A Transfer Learning Analysis of The Deep Deterministic Policy Gradient Method

## Hamin Cheon, University of Groningen

Deep Deterministic Policy Gradient (DDPG) is a type of Deep Reinforcement Learning (DRL) algorithm that is specialized in environments with continuous action space. One of the problems of DDPG, or Deep Learning (DL) in general, is that it is expensive to train. A possible solution to the problem is Transfer Learning (TL), a machine learning method that transfers the knowledge of a trained agent to another agent to give a quicker start on training. The paper investigates the effect of the memory buffer transfer and the model parameters transfer in training a DDPG agent. The three experiments, the regular Experience Replay (ER) buffer transfer, Prioritized Experience Replay (PER) buffer transfer, and model parameters transfer, were conducted to test the claim. The agents were trained in two OpenAI gym environments. When the first agent was trained in the first environment, its knowledge was collected to transfer to the second agent. The new agent was then trained in the second environment that was different but related to the first environment. The results showed that the memory buffer transfer hurts the training of the DDPG agent, while the model parameters transfer benefits the training.
